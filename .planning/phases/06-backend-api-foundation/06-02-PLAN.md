---
phase: 06-backend-api-foundation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - helm-chart/file-simulator/templates/control-api.yaml
  - helm-chart/file-simulator/templates/control-api-rbac.yaml
  - helm-chart/file-simulator/values.yaml
autonomous: true

must_haves:
  truths:
    - "helm template renders control-api deployment without errors"
    - "ServiceAccount has RBAC permissions to read pods/services/deployments"
    - "Deployment has resource limits (256Mi request, 1Gi limit)"
    - "Service exposes both HTTP (5000) and NodePort for external access"
    - "Control API does not evict existing v1.0 pods (resource budget maintained)"
  artifacts:
    - path: "helm-chart/file-simulator/templates/control-api.yaml"
      provides: "Deployment and Service for control API"
      contains: "file-simulator-control-api"
    - path: "helm-chart/file-simulator/templates/control-api-rbac.yaml"
      provides: "RBAC configuration for Kubernetes API access"
      contains: "ClusterRole"
    - path: "helm-chart/file-simulator/values.yaml"
      provides: "Control API configuration section"
      contains: "controlApi:"
  key_links:
    - from: "control-api.yaml"
      to: "control-api-rbac.yaml"
      via: "serviceAccountName reference"
      pattern: "serviceAccountName:.*control-api"
    - from: "control-api.yaml"
      to: "values.yaml"
      via: "Helm value references"
      pattern: "\\.Values\\.controlApi"
---

<objective>
Create Kubernetes RBAC configuration and Helm templates for deploying the Control API alongside existing v1.0 servers.

Purpose: Establish Kubernetes deployment infrastructure with proper RBAC permissions for the control plane to query pods, services, and deployments via the Kubernetes API. Resource limits ensure the control API coexists with 7 NAS + 6 protocol servers without evictions.

Output: Helm templates ready to deploy control-api pod with verified RBAC permissions.
</objective>

<execution_context>
@C:\Users\UserC\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\UserC\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-backend-api-foundation/06-CONTEXT.md
@helm-chart/file-simulator/values.yaml
@helm-chart/file-simulator/templates/_helpers.tpl
@helm-chart/file-simulator/templates/serviceaccount.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Control API Configuration to values.yaml</name>
  <files>helm-chart/file-simulator/values.yaml</files>
  <action>
Add new `controlApi` section to values.yaml AFTER the existing `monitoring` section (around line 360) and BEFORE `ingress`:

```yaml
# ============================================
# Control API - v2.0 Control Plane
# ============================================
controlApi:
  enabled: true

  # Container image (will be built from src/FileSimulator.ControlApi)
  image:
    repository: file-simulator-control-api
    tag: latest
    pullPolicy: IfNotPresent

  # Service configuration
  service:
    type: NodePort
    httpPort: 5000
    nodePort: 30500  # External access from Windows

  # RBAC configuration
  rbac:
    create: true
    # Permissions needed for Phase 6 (read-only discovery)
    # Phase 11 will add create/update/delete for dynamic management
    rules:
      - apiGroups: [""]
        resources: ["pods", "services", "configmaps"]
        verbs: ["get", "list", "watch"]
      - apiGroups: ["apps"]
        resources: ["deployments"]
        verbs: ["get", "list", "watch"]

  # Resource limits - sized to coexist with v1.0 servers
  # Current cluster: 8GB RAM, 4 CPU
  # v1.0 usage: ~706Mi request, ~2.85Gi limit
  # Control API: adds 256Mi request, 1Gi limit (within budget)
  resources:
    requests:
      memory: "256Mi"
      cpu: "200m"
    limits:
      memory: "1Gi"
      cpu: "1"

  # Health check configuration
  healthCheck:
    path: /health
    initialDelaySeconds: 10
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 3

  # SignalR configuration
  signalr:
    # Hub path for WebSocket connections
    hubPath: /hubs/status
    # Keep-alive interval (seconds)
    keepAliveInterval: 15
```

**Resource calculation (from CONTEXT.md):**
- v1.0 servers: 706Mi request, 2.85Gi limit
- Control API: 256Mi request, 1Gi limit
- Total: 962Mi request (~12% of 8GB), 3.85Gi limit (~48% of 8GB)
- Sufficient headroom for Phase 7-9 additions

**NodePort 30500:**
- Chosen to avoid conflicts with existing services (30021 FTP, 30022 SFTP, etc.)
- Allows Windows host access: http://$(minikube -p file-simulator ip):30500
  </action>
  <verify>
```bash
grep -A 30 "controlApi:" helm-chart/file-simulator/values.yaml | head -35
```
Should show controlApi section with enabled, image, service, rbac, resources keys.

Validate YAML syntax:
```bash
helm lint helm-chart/file-simulator
```
Should pass linting.
  </verify>
  <done>
controlApi section added to values.yaml with image config, NodePort 30500, RBAC rules for read-only K8s access, resource limits (256Mi/1Gi), and health check configuration.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create RBAC Template (ServiceAccount, Role, RoleBinding)</name>
  <files>helm-chart/file-simulator/templates/control-api-rbac.yaml</files>
  <action>
Create control-api-rbac.yaml with Kubernetes RBAC resources:

```yaml
{{- if and .Values.controlApi.enabled .Values.controlApi.rbac.create }}
---
# ServiceAccount for Control API
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "file-simulator.fullname" . }}-control-api
  namespace: {{ include "file-simulator.namespace" . }}
  labels:
    {{- include "file-simulator.labels" . | nindent 4 }}
    app.kubernetes.io/component: control-api

---
# Role with permissions to read cluster resources
# Scoped to file-simulator namespace for security
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: {{ include "file-simulator.fullname" . }}-control-api
  namespace: {{ include "file-simulator.namespace" . }}
  labels:
    {{- include "file-simulator.labels" . | nindent 4 }}
    app.kubernetes.io/component: control-api
rules:
  {{- toYaml .Values.controlApi.rbac.rules | nindent 2 }}

---
# RoleBinding connecting ServiceAccount to Role
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: {{ include "file-simulator.fullname" . }}-control-api
  namespace: {{ include "file-simulator.namespace" . }}
  labels:
    {{- include "file-simulator.labels" . | nindent 4 }}
    app.kubernetes.io/component: control-api
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: {{ include "file-simulator.fullname" . }}-control-api
subjects:
  - kind: ServiceAccount
    name: {{ include "file-simulator.fullname" . }}-control-api
    namespace: {{ include "file-simulator.namespace" . }}
{{- end }}
```

**Design decisions:**
- Use Role (not ClusterRole) - scoped to file-simulator namespace only
- ServiceAccount name includes "-control-api" suffix to distinguish from main file-simulator-sa
- Labels include component selector for easy querying
- Rules come from values.yaml for flexibility (Phase 11 can add create/update/delete)

**Why Role instead of ClusterRole:**
- Control API only needs to discover servers in file-simulator namespace
- Principle of least privilege
- Easier RBAC troubleshooting (namespace-scoped)
  </action>
  <verify>
```bash
helm template file-sim ./helm-chart/file-simulator --debug 2>&1 | grep -A 20 "kind: Role"
```
Should show Role resource with correct rules.

```bash
helm template file-sim ./helm-chart/file-simulator --debug 2>&1 | grep -A 10 "kind: RoleBinding"
```
Should show RoleBinding connecting ServiceAccount to Role.
  </verify>
  <done>
control-api-rbac.yaml created with ServiceAccount, Role, and RoleBinding. RBAC is scoped to file-simulator namespace with read-only permissions for pods/services/deployments/configmaps.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Deployment and Service Template</name>
  <files>helm-chart/file-simulator/templates/control-api.yaml</files>
  <action>
Create control-api.yaml with Deployment and Service:

```yaml
{{- if .Values.controlApi.enabled }}
---
# Deployment for Control API
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "file-simulator.fullname" . }}-control-api
  namespace: {{ include "file-simulator.namespace" . }}
  labels:
    {{- include "file-simulator.labels" . | nindent 4 }}
    app.kubernetes.io/component: control-api
spec:
  replicas: 1
  selector:
    matchLabels:
      {{- include "file-simulator.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/component: control-api
  template:
    metadata:
      labels:
        {{- include "file-simulator.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/component: control-api
      annotations:
        # Force pod restart on config changes
        checksum/config: {{ include (print $.Template.BasePath "/control-api-rbac.yaml") . | sha256sum }}
    spec:
      serviceAccountName: {{ include "file-simulator.fullname" . }}-control-api
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      containers:
        - name: control-api
          image: "{{ .Values.controlApi.image.repository }}:{{ .Values.controlApi.image.tag }}"
          imagePullPolicy: {{ .Values.controlApi.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 5000
              protocol: TCP
          env:
            - name: ASPNETCORE_URLS
              value: "http://+:5000"
            - name: ASPNETCORE_ENVIRONMENT
              value: "Production"
            - name: Kubernetes__InCluster
              value: "true"
            - name: Kubernetes__Namespace
              value: {{ include "file-simulator.namespace" . | quote }}
          livenessProbe:
            httpGet:
              path: {{ .Values.controlApi.healthCheck.path }}
              port: http
            initialDelaySeconds: {{ .Values.controlApi.healthCheck.initialDelaySeconds }}
            periodSeconds: {{ .Values.controlApi.healthCheck.periodSeconds }}
            timeoutSeconds: {{ .Values.controlApi.healthCheck.timeoutSeconds }}
            failureThreshold: {{ .Values.controlApi.healthCheck.failureThreshold }}
          readinessProbe:
            httpGet:
              path: {{ .Values.controlApi.healthCheck.path }}
              port: http
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 3
          resources:
            {{- toYaml .Values.controlApi.resources | nindent 12 }}
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: false  # .NET needs to write temp files
            capabilities:
              drop:
                - ALL
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}

---
# Service for Control API
apiVersion: v1
kind: Service
metadata:
  name: {{ include "file-simulator.fullname" . }}-control-api
  namespace: {{ include "file-simulator.namespace" . }}
  labels:
    {{- include "file-simulator.labels" . | nindent 4 }}
    app.kubernetes.io/component: control-api
spec:
  type: {{ .Values.controlApi.service.type }}
  ports:
    - name: http
      port: {{ .Values.controlApi.service.httpPort }}
      targetPort: http
      protocol: TCP
      {{- if eq .Values.controlApi.service.type "NodePort" }}
      nodePort: {{ .Values.controlApi.service.nodePort }}
      {{- end }}
  selector:
    {{- include "file-simulator.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/component: control-api
{{- end }}
```

**Key configuration:**
- serviceAccountName references RBAC ServiceAccount for K8s API access
- securityContext runs as non-root (user 1000 matching Dockerfile)
- Environment variables configure Kestrel and K8s client
- Liveness/readiness probes use /health endpoint
- readOnlyRootFilesystem: false because .NET writes temp files
- NodePort 30500 for Windows host access

**Checksum annotation:**
- Triggers pod restart when RBAC changes
- Standard Helm pattern for config reload
  </action>
  <verify>
```bash
helm template file-sim ./helm-chart/file-simulator --debug 2>&1 | grep -A 50 "kind: Deployment" | grep -A 50 "control-api" | head -60
```
Should show Deployment with serviceAccountName, security context, probes.

```bash
helm template file-sim ./helm-chart/file-simulator --debug 2>&1 | grep -B 2 -A 15 "kind: Service" | grep -A 15 "control-api"
```
Should show Service with NodePort 30500.

Full template validation:
```bash
helm template file-sim ./helm-chart/file-simulator --debug 2>&1 | tail -5
```
Should not show any template errors.
  </verify>
  <done>
control-api.yaml created with Deployment (non-root, RBAC service account, liveness/readiness probes, resource limits) and Service (NodePort 30500). Template renders without errors.
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Helm lint passes:
   ```bash
   helm lint helm-chart/file-simulator
   ```

2. Full template renders without errors:
   ```bash
   helm template file-sim ./helm-chart/file-simulator --debug 2>&1 | tail -10
   ```

3. RBAC resources present:
   ```bash
   helm template file-sim ./helm-chart/file-simulator | grep "kind:" | sort | uniq -c
   ```
   Should include Role, RoleBinding, ServiceAccount for control-api.

4. Resource limits verify budget:
   ```bash
   helm template file-sim ./helm-chart/file-simulator | grep -A 5 "resources:" | grep -E "(memory|cpu)"
   ```
   Should show 256Mi/1Gi for control-api.

5. NodePort 30500 assigned:
   ```bash
   helm template file-sim ./helm-chart/file-simulator | grep "nodePort: 30500"
   ```
</verification>

<success_criteria>
1. values.yaml contains controlApi section with enabled, image, service, rbac, resources
2. control-api-rbac.yaml creates ServiceAccount, Role, RoleBinding
3. Role has read permissions for pods, services, deployments, configmaps
4. control-api.yaml creates Deployment with serviceAccountName reference
5. Deployment has liveness/readiness probes on /health
6. Service is NodePort type on port 30500
7. Resource limits are 256Mi request, 1Gi limit (within cluster budget)
8. helm lint passes without errors
9. helm template renders all resources correctly
</success_criteria>

<output>
After completion, create `.planning/phases/06-backend-api-foundation/06-02-SUMMARY.md`
</output>
