---
phase: 09-historical-metrics-storage
plan: 06
type: execute
wave: 4
depends_on: ["09-02", "09-04", "09-05"]
files_modified: []
autonomous: false

must_haves:
  truths:
    - "SQLite database persists health metrics with 5-second granularity"
    - "Historical trends dashboard shows connection latency over time"
    - "User can query metrics for specific time ranges (1h, 24h, 7d)"
    - "Database survives pod restarts with data intact"
    - "Sparklines show latency trends on server cards"
---

<objective>
Human verification checkpoint for Phase 9 - validate complete historical metrics functionality.

Purpose: Verify all success criteria are met before marking phase complete.
Output: Verified functionality, ready for next phase.
</objective>

<execution_context>
@C:\Users\UserC\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\UserC\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/09-historical-metrics-storage/09-CONTEXT.md
</context>

<tasks>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete historical metrics system with:
- SQLite database for metrics persistence (health_samples + health_hourly tables)
- Background services for sample recording, hourly rollup generation, 7-day retention cleanup
- REST API endpoints (/api/metrics/samples, /api/metrics/hourly, /api/metrics/servers)
- SignalR hub (/hubs/metrics) for real-time sample streaming
- React dashboard with History tab, zoomable latency charts, and time range selection
- Mini sparklines on server cards showing last hour of latency
- PVC/hostPath for database persistence across pod restarts
  </what-built>

  <how-to-verify>
**Prerequisites:**
1. Ensure Minikube file-simulator profile is running
2. Deploy updated Helm chart with new control-api image
3. Start dashboard dev server: `cd src/dashboard && npm run dev`

**Test 1: Database Persistence**
1. SSH into control-api pod: `kubectl --context=file-simulator exec -it deploy/file-sim-file-simulator-control-api -n file-simulator -- /bin/sh`
2. Check database exists: `ls -la /mnt/control-data/`
3. Should see metrics.db file
4. Exit pod, delete it: `kubectl --context=file-simulator delete pod -l app.kubernetes.io/component=control-api -n file-simulator`
5. Wait for pod restart, re-enter and verify metrics.db still exists with data

**Test 2: Sample Recording (wait 1-2 minutes after startup)**
1. Open dashboard at http://localhost:5173
2. Go to Servers tab - should see 13 servers with health status
3. Check browser network tab for /api/metrics/samples or /api/metrics/servers
4. API should return samples (may be empty initially - wait 1-2 minutes for samples to accumulate)

**Test 3: History Tab Functionality**
1. Click "History" tab in navigation
2. Should see time range selector with preset buttons (1h, 6h, 24h, 7d)
3. Default view should be "Last 24 hours"
4. Chart should show latency lines for all servers (or empty state if no data yet)
5. Try drag-selecting a region on the chart to zoom
6. Click "Reset Zoom" button - should zoom out

**Test 4: Time Range Selection**
1. Click "1h" preset button - chart should reload with last hour data
2. Click "7d" preset button - chart should switch to hourly resolution
3. Use dropdown to select "30 minutes" - should load raw samples
4. Verify resolution badge changes: "Raw (5s)" for short ranges, "Hourly" for 24h+

**Test 5: Sparklines on Server Cards**
1. Go to Servers tab
2. After metrics accumulate (1-2 minutes), server cards should show mini sparklines
3. Sparklines should be colored green for healthy servers, red for unhealthy
4. Click on a sparkline - should navigate to History tab filtered to that server

**Test 6: Server Filtering**
1. In History tab, use server dropdown to select a specific server (e.g., "nas-input-1")
2. Chart should show only that server's latency line
3. Select "All Servers" - should show all lines again

**Test 7: API Endpoints**
Using curl or browser:
1. GET http://172.25.174.184:30500/api/metrics/servers
   - Should return list of servers with metrics date range
2. GET http://172.25.174.184:30500/api/metrics/samples?startTime=2024-01-01T00:00:00Z&endTime=2024-12-31T23:59:59Z
   - Should return raw samples (may be empty if new deployment)
3. GET http://172.25.174.184:30500/api/metrics/hourly?startTime=2024-01-01T00:00:00Z&endTime=2024-12-31T23:59:59Z
   - Should return hourly aggregations (may be empty until rollups run)

**Expected Results:**
- All 5 success criteria from ROADMAP.md must be TRUE
- No JavaScript errors in browser console
- No errors in control-api pod logs (check with kubectl logs)
  </how-to-verify>

  <resume-signal>
Type "approved" if all verification tests pass.

If issues found, describe:
1. Which test failed
2. What was expected vs actual behavior
3. Any error messages from browser console or pod logs
  </resume-signal>
</task>

</tasks>

<verification>
Human has verified:
1. Database persists with health metrics at 5-second granularity
2. Historical trends dashboard shows latency over time
3. Time range selection works (1h, 6h, 24h, 7d)
4. Database survives pod restarts
5. Auto-cleanup (will verify after 7 days, or check logs for cleanup service running)
</verification>

<success_criteria>
All Phase 9 success criteria verified:
- [ ] SQLite database persists health metrics with 5-second granularity for 7 days
- [ ] Historical trends dashboard shows connection counts, latency, and errors over time
- [ ] User can query metrics for specific time ranges (last 1h, last 24h, last 7d)
- [ ] Database survives pod restarts with data intact
- [ ] Auto-cleanup removes data older than 7 days to prevent unbounded growth
</success_criteria>

<output>
After approval, create `.planning/phases/09-historical-metrics-storage/09-06-SUMMARY.md`

Update `.planning/STATE.md`:
- Phase: 9 of 12 - COMPLETE
- Plan: 6/6 complete
- Status: UAT passed

Update `.planning/ROADMAP.md`:
- Phase 9 status: Complete
</output>
