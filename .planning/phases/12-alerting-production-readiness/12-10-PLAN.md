# Phase 12 Plan 10: Human Verification Checkpoint

## Goal
Validate complete Phase 12 implementation including alerts, containerization, production deployment, and verification scripts.

## Type
checkpoint:human-verify

## What Was Built

### Backend Alert System (Plans 01-02)
- AlertService with disk space and Kafka health checks
- Alert persistence in SQLite with 7-day retention
- SignalR AlertHub for real-time notifications
- REST API endpoints for alert queries
- Optional Redis backplane for scale-out (disabled by default)

### Frontend Alert UI (Plans 03-04)
- Sonner toast notifications with severity-based durations
- Persistent alert banner showing unresolved counts
- Dedicated Alerts tab with filtering and search
- Error boundaries preventing component crashes
- Integration into main App component

### Production Containerization (Plans 05-06)
- Multi-stage Dashboard Dockerfile (Node → nginx)
- nginx configuration with /health endpoint and SPA routing
- Dashboard Helm deployment template
- NFS fix incorporated into nas.yaml (emptyDir pattern)
- Local registry integration for custom images

### Deployment Automation (Plans 07-09)
- Deploy-Production.ps1 comprehensive deployment script
- Verify-Production.ps1 with 37+ verification tests
- Setup-Hosts.ps1 enhancements (Kafka + dynamic NAS)
- Build automation for Control API and Dashboard images
- Health check validation for all components

## How to Verify

### Step 1: Deploy Production Environment
```powershell
# Open PowerShell as Administrator
cd C:\Users\UserC\source\repos\file-simulator-suite

# Deploy from scratch (creates cluster, builds images, deploys Helm)
.\scripts\Deploy-Production.ps1 -Clean

# Expected output:
# - [1/9] Creating fresh cluster... ✓
# - [2/9] Setting up container registry... ✓
# - [3/9] Building Control API image... ✓
# - [4/9] Building Dashboard image... ✓
# - [5/9] Pushing images to registry... ✓
# - [6/9] Deploying Helm chart... ✓
# - [7/9] Updating hosts file... ✓
# - [8/9] Verifying deployment... ✓
# - [9/9] Cleaning up... ✓
# === Deployment Complete ===
# Dashboard: http://file-simulator.local:30080
# Control API: http://file-simulator.local:30500

# Verify deployment (~5 minutes)
.\scripts\Verify-Production.ps1

# Expected output:
# === Kubernetes Cluster ===
# ✓ Cluster running
# ✓ All pods Running (13/13)
# === Management UI ===
# ✓ Dashboard health endpoint
# ✓ Control API health endpoint
# === Protocol Servers ===
# ✓ All protocols accessible
# === Summary ===
# Total: 37, Passed: 37, Failed: 0
# Pass Rate: 100.0%
# ✓ All tests passed - Production ready!
```

### Step 2: Test Dashboard Alerts
```powershell
# Open browser to dashboard
start http://file-simulator.local:30080

# Visual checks:
1. Dashboard loads without errors (React app renders)
2. AlertBanner NOT visible initially (no unresolved alerts)
3. Navigate to Alerts tab (5th tab after Servers/Files/History/Kafka)
4. Alerts tab shows empty state: "No alerts found"
5. Alert stats show all zeros (Total: 0, Critical: 0, Warning: 0, Info: 0)

# Trigger disk space alert (if C:\simulator-data has <1GB free)
# Or test by changing threshold:
# Edit src/FileSimulator.ControlApi/Services/AlertService.cs
# Change DiskSpaceThresholdBytes to very high value (e.g., 100GB)
# Rebuild and redeploy Control API

# Expected behavior:
1. Toast notification appears in bottom-right corner (Warning severity)
2. Toast shows: "Low Disk Space" with available space message
3. Toast auto-dismisses after 10 seconds
4. AlertBanner appears at top showing: "1 Warning"
5. AlertBanner styled with orange background
6. Alerts tab shows 1 unresolved alert in table
7. Alert row shows: Warning badge, DiskSpace type, triggered timestamp

# Test alert resolution:
# Free up disk space or change threshold back
# Wait 30 seconds for next check cycle

# Expected behavior:
1. AlertBanner disappears (no unresolved alerts)
2. Alerts tab shows alert with "Resolved" status and gray color
3. Alert has resolvedAt timestamp
```

### Step 3: Test Error Boundaries
```powershell
# Force an error in a tab component
# Open browser DevTools (F12)
# In Console, run:
window.__THROW_ERROR_IN_SERVERS_TAB = true;

# Navigate to Servers tab

# Expected behavior:
1. Servers tab shows ErrorFallback component
2. Error message displayed: "Something went wrong"
3. "Try Again" button visible
4. "Reload Page" button visible
5. Other tabs still functional (Files, History, Kafka, Alerts)
6. Header and navigation still responsive
7. Dashboard does NOT crash or show white screen

# Click "Try Again" button
# Expected: Error boundary resets, attempts to render Servers tab again

# Click "Reload Page" button
# Expected: Full page reload, dashboard loads normally
```

### Step 4: Test Container Images
```powershell
# Check image sizes
docker images | grep file-simulator

# Expected output (approximate sizes):
# file-simulator-control-api    latest    ...    ~200MB
# file-simulator-dashboard      latest    ...    ~30MB

# Test Dashboard container locally
docker run -d -p 8080:80 --name test-dashboard localhost:5000/file-simulator-dashboard:latest
curl http://localhost:8080/health
# Expected: "healthy"

curl http://localhost:8080/
# Expected: HTML content with <!DOCTYPE html>

curl http://localhost:8080/servers
# Expected: Same HTML (SPA routing works, not 404)

# Cleanup
docker stop test-dashboard && docker rm test-dashboard
```

### Step 5: Test NFS Fix
```powershell
# Check NAS pod logs (should NOT see "exportfs: /data does not support NFS export")
kubectl --context=file-simulator logs -n file-simulator deployment/file-sim-file-simulator-nas --tail=50

# Expected: No NFS export errors, successful startup logs

# Verify NAS volumes
kubectl --context=file-simulator get pod -n file-simulator -l app.kubernetes.io/component=nas -o yaml | Select-String -Pattern "emptyDir|persistentVolumeClaim"

# Expected output:
# - name: nfs-data
#   emptyDir: {}
# - name: shared-data
#   persistentVolumeClaim:
#     claimName: file-sim-file-simulator-pvc

# Test NFS mount (from application pod perspective)
# This validates NFS exports work correctly
kubectl --context=file-simulator run -it --rm nfs-test --image=busybox --restart=Never -- sh -c "mount -t nfs file-sim-file-simulator-nas:/ /mnt && ls /mnt && echo SUCCESS"

# Expected: SUCCESS (mount works, directory listing shows files)
```

### Step 6: Test Dynamic Server with Hosts Update
```powershell
# Create dynamic NAS server via dashboard
# Open http://file-simulator.local:30080
# Navigate to Servers tab
# Click "+ Add Server" → Select "NAS Server"
# Fill: name="test-nas", directory="/test-data"
# Click "Create"
# Wait 30 seconds for deployment

# Update hosts file with dynamic entry
.\scripts\Setup-Hosts.ps1 -IncludeDynamic

# Expected output:
# Checking for dynamic servers...
#   Added dynamic NAS: nas-test-nas.file-simulator.local
# Added 4 static hostnames, 1 dynamic hostnames
# Hosts file updated successfully

# Test dynamic NAS hostname
Test-NetConnection nas-test-nas.file-simulator.local -Port 32150

# Expected: TcpTestSucceeded = True

# Cleanup: Delete test-nas server from dashboard
```

### Step 7: Test Redis Backplane (Optional)
```powershell
# Enable Redis in Helm values
# Edit helm-chart/file-simulator/values.yaml
# Change redis.enabled: false → true

# Redeploy
helm upgrade --install file-sim ./helm-chart/file-simulator --kube-context=file-simulator --namespace file-simulator

# Wait for Redis pod
kubectl --context=file-simulator get pods -n file-simulator -w

# Check Control API logs for Redis connection
kubectl --context=file-simulator logs -n file-simulator deployment/file-sim-file-simulator-control-api --tail=20

# Expected: "SignalR backplane: Redis" in logs

# Scale Control API to 2 replicas
kubectl --context=file-simulator scale deployment/file-sim-file-simulator-control-api -n file-simulator --replicas=2

# Open two browser windows to dashboard
# Trigger alert in one window
# Expected: Both windows receive toast notification (Redis broadcasts)

# Cleanup: Scale back to 1 replica, disable Redis
kubectl --context=file-simulator scale deployment/file-sim-file-simulator-control-api -n file-simulator --replicas=1
# Edit values.yaml redis.enabled: true → false, redeploy
```

## Expected Outcomes

### Alerting System
- [ ] Disk space alerts trigger when below 1GB threshold
- [ ] Kafka alerts trigger when broker unreachable
- [ ] Server health alerts trigger on consecutive failures
- [ ] Toast notifications appear with correct severity colors and durations
- [ ] Alert banner shows unresolved alert counts at top
- [ ] Alerts tab displays filterable alert history
- [ ] Alerts auto-resolve when conditions clear
- [ ] Alert retention cleanup removes alerts older than 7 days

### Error Handling
- [ ] Error boundaries prevent single component crashes
- [ ] ErrorFallback displays user-friendly error messages
- [ ] "Try Again" button resets error boundary
- [ ] Other tabs remain functional when one fails
- [ ] Dashboard never shows white screen of death

### Containerization
- [ ] Dashboard Docker image builds successfully (~30MB)
- [ ] Control API Docker image builds successfully (~200MB)
- [ ] nginx /health endpoint returns "healthy"
- [ ] SPA routing works (no 404 on direct navigation)
- [ ] Static asset caching headers present (1 year)
- [ ] HTML no-cache headers prevent stale content
- [ ] gzip compression active for JS/CSS files

### NFS Fix
- [ ] NAS pods start without export errors
- [ ] emptyDir volume used for /data (NFS export)
- [ ] PVC volume used for /shared (persistent storage)
- [ ] NFS mounts work from application pods
- [ ] No manual nfs-fix-patch.yaml needed

### Deployment Automation
- [ ] Deploy-Production.ps1 completes successfully with -Clean flag
- [ ] All images build and push to local registry
- [ ] Helm chart deploys with --wait successfully
- [ ] Hosts file updates with static and dynamic entries
- [ ] Verify-Production.ps1 reports 100% pass rate
- [ ] Setup-Hosts.ps1 discovers dynamic servers with -IncludeDynamic

### Production Readiness
- [ ] All 13 pods Running (7 NAS + 6 protocols + Kafka + Control API + Dashboard)
- [ ] All services accessible via file-simulator.local DNS
- [ ] Dashboard accessible at port 30080
- [ ] Control API accessible at port 30500
- [ ] Kafka accessible at kafka.file-simulator.local:30094
- [ ] Dynamic NAS servers accessible at nas-{name}.file-simulator.local
- [ ] Health checks passing for all components

## Issues to Report

If any verification fails:
1. Copy full error message and stack trace
2. Include relevant pod logs: `kubectl --context=file-simulator logs <pod> -n file-simulator`
3. Include browser console errors (F12 DevTools)
4. Include script output showing failure point
5. Take screenshot if UI issue
6. Report which step failed and observed vs expected behavior

Type "approved" if all verifications pass, or describe issues if any fail.

## Resume Signal
Type "approved" once verification complete, or describe issues found.
