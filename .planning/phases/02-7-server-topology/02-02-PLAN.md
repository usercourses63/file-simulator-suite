---
phase: 02-7-server-topology
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/test-multi-nas.ps1
autonomous: false

must_haves:
  truths:
    - "All 7 NAS pods are Running status in Minikube"
    - "Each NAS server accessible via unique NodePort from Windows host"
    - "Files in nas-input-1 NOT visible in nas-input-2 (storage isolation)"
    - "Each NAS has unique DNS name resolvable within cluster"
    - "All 7 servers fit within Minikube 8GB/4CPU resource constraints"
  artifacts:
    - path: "scripts/test-multi-nas.ps1"
      provides: "Automated validation script for 7-server topology"
      min_lines: 100
      contains: "nas-input-1"
  key_links:
    - from: "kubectl get pods"
      to: "7 NAS deployments"
      via: "selector labels"
      pattern: "simulator.protocol=nfs"
    - from: "Windows C:\\simulator-data\\*"
      to: "Pod /data directories"
      via: "init container rsync"
      pattern: "rsync.*windows-mount.*nfs-data"
---

<objective>
Deploy the 7-server NAS topology to Minikube and validate storage isolation, DNS resolution, and resource constraints.

Purpose: Prove the multi-instance pattern works at scale (7 servers) within Minikube resource limits, with each NAS completely isolated from others.

Output: 7 running NAS pods with validated storage isolation + test-multi-nas.ps1 automation script.
</objective>

<execution_context>
@C:\Users\UserC\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\UserC\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-7-server-topology/02-RESEARCH.md
@.planning/phases/02-7-server-topology/02-01-SUMMARY.md

# Phase 1 test script pattern
@scripts/test-nas-pattern.ps1

# Values file to deploy
@helm-chart/file-simulator/values-multi-nas.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deploy 7 NAS Servers to Minikube</name>
  <files>None (deployment only)</files>
  <action>
Deploy the 7-server NAS topology using Helm.

**Pre-deployment checks:**
```bash
# Verify Minikube is running with mount
minikube status

# Check available NodePorts (32150-32156 should be free)
kubectl get svc --all-namespaces | grep -E "321[5-6][0-6]"
```

**Create Windows directories for all 7 NAS servers:**
```powershell
$nasServers = @("nas-input-1", "nas-input-2", "nas-input-3", "nas-backup", "nas-output-1", "nas-output-2", "nas-output-3")
foreach ($nas in $nasServers) {
    New-Item -ItemType Directory -Force -Path "C:\simulator-data\$nas"
    Set-Content -Path "C:\simulator-data\$nas\README.txt" -Value "This directory belongs to $nas"
}
```

**Deploy with Helm:**
```bash
helm upgrade --install file-sim ./helm-chart/file-simulator \
  -f ./helm-chart/file-simulator/values-multi-nas.yaml \
  --namespace file-simulator \
  --create-namespace
```

**Wait for all pods to be ready:**
```bash
kubectl wait --for=condition=Ready pod -l simulator.protocol=nfs -n file-simulator --timeout=180s
```

**Verify deployment:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs
# Should show 7 pods all in Running state
```

**If any pods fail to start:**
1. Check pod events: `kubectl describe pod <pod-name> -n file-simulator`
2. Check init container logs: `kubectl logs <pod-name> -c sync-windows-data -n file-simulator`
3. Check main container logs: `kubectl logs <pod-name> -c nfs-server -n file-simulator`
4. Verify Windows directories exist: `ls C:\simulator-data\`
  </action>
  <verify>
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o wide
# All 7 pods should be Running with 1/1 READY

kubectl get svc -n file-simulator -l simulator.protocol=nfs
# All 7 services should exist with NodePorts
```
  </verify>
  <done>All 7 NAS pods (nas-input-1/2/3, nas-backup, nas-output-1/2/3) running with 1/1 READY status</done>
</task>

<task type="auto">
  <name>Task 2: Validate Storage Isolation Between NAS Servers</name>
  <files>None (validation only)</files>
  <action>
Verify that each NAS server sees only its own files - no cross-contamination.

**Create unique test files on Windows (one per NAS):**
```powershell
$nasServers = @("nas-input-1", "nas-input-2", "nas-input-3", "nas-backup", "nas-output-1", "nas-output-2", "nas-output-3")
foreach ($nas in $nasServers) {
    Set-Content -Path "C:\simulator-data\$nas\isolation-test-$nas.txt" -Value "This file should ONLY be visible in $nas"
}
```

**Restart all NAS pods to trigger init container sync:**
```bash
kubectl delete pods -n file-simulator -l simulator.protocol=nfs
kubectl wait --for=condition=Ready pod -l simulator.protocol=nfs -n file-simulator --timeout=180s
```

**Verify each pod sees only its own test file:**
```bash
# For each NAS server, count txt files in /data
# Each should see exactly 2 files: README.txt + isolation-test-{name}.txt

for nas in nas-input-1 nas-input-2 nas-input-3 nas-backup nas-output-1 nas-output-2 nas-output-3; do
  echo "=== $nas ==="
  POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=$nas -o jsonpath='{.items[0].metadata.name}')
  kubectl exec -n file-simulator $POD -- ls -la /data
  echo "File count: $(kubectl exec -n file-simulator $POD -- sh -c 'ls /data/*.txt 2>/dev/null | wc -l')"
  echo ""
done
```

**Isolation test (critical):**
- nas-input-1 should NOT see isolation-test-nas-input-2.txt
- Each pod should see exactly 2 .txt files (README + its own isolation test)

**If isolation fails:**
- Check hostPath in deployment: `kubectl get deployment -n file-simulator -o yaml | grep "path:"`
- Verify each deployment has unique path
  </action>
  <verify>
Each NAS pod shows exactly 2 .txt files (README.txt and its own isolation-test file).
No pod shows files from other NAS servers.
  </verify>
  <done>Storage isolation verified - each NAS sees only its own directory contents</done>
</task>

<task type="auto">
  <name>Task 3: Create Multi-NAS Validation Script</name>
  <files>scripts/test-multi-nas.ps1</files>
  <action>
Create comprehensive PowerShell test script for 7-server topology validation.

**Script structure (follow test-nas-pattern.ps1 pattern):**

```powershell
# test-multi-nas.ps1
# Phase 2: 7-Server NAS Topology Validation Script

param(
    [switch]$CreateTestFiles,
    [switch]$SkipDeployment
)

$ErrorActionPreference = "Stop"

$nasServers = @(
    @{name="nas-input-1"; nodePort=32150},
    @{name="nas-input-2"; nodePort=32151},
    @{name="nas-input-3"; nodePort=32152},
    @{name="nas-backup"; nodePort=32153},
    @{name="nas-output-1"; nodePort=32154},
    @{name="nas-output-2"; nodePort=32155},
    @{name="nas-output-3"; nodePort=32156}
)

function Write-Step { param($step, $description) Write-Host "`n=== Step $step`: $description ===" -ForegroundColor Cyan }
function Write-Pass { param($message) Write-Host "[PASS] $message" -ForegroundColor Green }
function Write-Fail { param($message) Write-Host "[FAIL] $message" -ForegroundColor Red }
function Write-Info { param($message) Write-Host "[INFO] $message" -ForegroundColor Yellow }
```

**Test steps to include:**
1. Minikube status check
2. Verify Windows directories exist (create if -CreateTestFiles)
3. Helm deployment status
4. All 7 pods running check
5. Init container success check (per pod)
6. Storage isolation test (critical)
7. DNS resolution test (from within cluster)
8. NodePort accessibility test
9. Resource usage check (kubectl top pods)
10. Summary with pass/fail count

**Critical tests:**
- Storage isolation (each pod sees only its directory)
- All 7 pods Running
- No privileged security context

**Output format:**
```
=== Step 1: Minikube Status ===
[PASS] Minikube is running
[PASS] Mount active: /mnt/simulator-data

=== Step 6: Storage Isolation ===
[PASS] nas-input-1: 2 files (isolated)
[PASS] nas-input-2: 2 files (isolated)
...

=== Summary ===
Passed: 28/30 tests
Failed: 2 tests
- DNS resolution requires rpcbind (deferred to Phase 3)
- External NFS mount not tested (Phase 2 limitation)
```
  </action>
  <verify>
```powershell
# Run the script
.\scripts\test-multi-nas.ps1

# Script should complete without errors
# All critical tests should pass
```
  </verify>
  <done>test-multi-nas.ps1 script created and validates 7-server topology</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
7-server NAS topology deployed to Minikube with:
- 7 independent NAS pods (nas-input-1/2/3, nas-backup, nas-output-1/2/3)
- Unique NodePorts (32150-32156) for each server
- Storage isolation (each NAS sees only its Windows directory)
- Non-privileged security context
- test-multi-nas.ps1 validation script
  </what-built>
  <how-to-verify>
1. Run the validation script:
   ```powershell
   .\scripts\test-multi-nas.ps1
   ```

2. Verify all 7 pods are running:
   ```bash
   kubectl get pods -n file-simulator -l simulator.protocol=nfs
   ```

3. Test storage isolation manually:
   - Create a file in C:\simulator-data\nas-input-1\test.txt
   - Restart nas-input-1 pod: `kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1`
   - Verify file appears: `kubectl exec -n file-simulator <pod> -- cat /data/test.txt`
   - Verify file NOT in nas-input-2: `kubectl exec -n file-simulator <nas-input-2-pod> -- ls /data/`

4. Check resource usage:
   ```bash
   kubectl top pods -n file-simulator -l simulator.protocol=nfs
   ```
   Total should be under 4GB RAM / 2 CPU (50% of Minikube capacity)

5. Verify NodePort accessibility (from Windows):
   ```powershell
   $minikubeIP = minikube ip
   # Each port should respond (may timeout without rpcbind, but port should be listening)
   Test-NetConnection -ComputerName $minikubeIP -Port 32150
   ```
  </how-to-verify>
  <resume-signal>Type "approved" if all 7 NAS servers are running with isolated storage, or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
**Deployment Status:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o wide
kubectl get svc -n file-simulator -l simulator.protocol=nfs
```

**Storage Isolation (must pass):**
```bash
# Each pod should see exactly its own files
for nas in nas-input-1 nas-input-2 nas-input-3 nas-backup nas-output-1 nas-output-2 nas-output-3; do
  POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=$nas -o jsonpath='{.items[0].metadata.name}')
  echo "$nas: $(kubectl exec -n file-simulator $POD -- sh -c 'ls /data/ | wc -l') files"
done
```

**Resource Usage:**
```bash
kubectl top pods -n file-simulator -l simulator.protocol=nfs
# Total should be under 500Mi RAM (well within 8GB Minikube)
```

**Security Context:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o jsonpath='{range .items[*]}{.metadata.name}: privileged={.spec.containers[0].securityContext.privileged}{"\n"}{end}'
# Should show no "privileged=true"
```
</verification>

<success_criteria>
1. All 7 NAS pods deployed and Running (1/1 READY)
2. Each NAS server has unique NodePort (32150-32156) accessible
3. Storage isolation verified (nas-input-1 files NOT visible in nas-input-2)
4. Each NAS has predictable DNS name within cluster
5. Total resource usage under 50% of Minikube capacity (4GB/2CPU)
6. No privileged security context on any NAS pod
7. test-multi-nas.ps1 script validates full topology
8. Human verification confirms system works as expected
</success_criteria>

<output>
After completion, create `.planning/phases/02-7-server-topology/02-02-SUMMARY.md`
</output>
