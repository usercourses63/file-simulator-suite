---
phase: 02-7-server-topology
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - scripts/test-multi-nas.ps1
autonomous: false

must_haves:
  truths:
    - "All 7 NAS pods are Running status in Minikube"
    - "Each NAS server accessible via unique NodePort from Windows host"
    - "Files in nas-input-1 NOT visible in nas-input-2 (storage isolation)"
    - "Each NAS has unique DNS name resolvable within cluster"
    - "All 7 servers fit within Minikube 8GB/4CPU resource constraints"
    - "Subdirectories in Windows visible via NFS mount (EXP-01)"
    - "Runtime-created subdirectories persist across pod restarts (EXP-02)"
    - "Single pod can mount multiple NAS servers simultaneously (INT-03)"
  artifacts:
    - path: "scripts/test-multi-nas.ps1"
      provides: "Automated validation script for 7-server topology"
      min_lines: 100
      contains: "nas-input-1"
  key_links:
    - from: "kubectl get pods"
      to: "7 NAS deployments"
      via: "selector labels"
      pattern: "simulator.protocol=nfs"
    - from: "Windows C:\\simulator-data\\*"
      to: "Pod /data directories"
      via: "init container rsync"
      pattern: "rsync.*windows-mount.*nfs-data"
---

<objective>
Deploy the 7-server NAS topology to Minikube and validate storage isolation, DNS resolution, subdirectory support, and resource constraints.

Purpose: Prove the multi-instance pattern works at scale (7 servers) within Minikube resource limits, with each NAS completely isolated from others, supporting subdirectories and multi-mount scenarios.

Output: 7 running NAS pods with validated storage isolation + test-multi-nas.ps1 automation script.
</objective>

<execution_context>
@C:\Users\UserC\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\UserC\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-7-server-topology/02-RESEARCH.md
@.planning/phases/02-7-server-topology/02-01-SUMMARY.md

# Phase 1 test script pattern
@scripts/test-nas-pattern.ps1

# Values file to deploy
@helm-chart/file-simulator/values-multi-nas.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Deploy 7 NAS Servers to Minikube</name>
  <files>None (deployment only)</files>
  <action>
Deploy the 7-server NAS topology using Helm.

**Pre-deployment checks:**
```bash
# Verify Minikube is running with mount
minikube status

# Check available NodePorts (32150-32156 should be free)
kubectl get svc --all-namespaces | grep -E "321[5-6][0-6]"
```

**Create Windows directories for all 7 NAS servers:**
```powershell
$nasServers = @("nas-input-1", "nas-input-2", "nas-input-3", "nas-backup", "nas-output-1", "nas-output-2", "nas-output-3")
foreach ($nas in $nasServers) {
    New-Item -ItemType Directory -Force -Path "C:\simulator-data\$nas"
    Set-Content -Path "C:\simulator-data\$nas\README.txt" -Value "This directory belongs to $nas"
}
```

**Deploy with Helm:**
```bash
helm upgrade --install file-sim ./helm-chart/file-simulator \
  -f ./helm-chart/file-simulator/values-multi-nas.yaml \
  --namespace file-simulator \
  --create-namespace
```

**Wait for all pods to be ready:**
```bash
kubectl wait --for=condition=Ready pod -l simulator.protocol=nfs -n file-simulator --timeout=180s
```

**Verify deployment:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs
# Should show 7 pods all in Running state
```

**If any pods fail to start:**
1. Check pod events: `kubectl describe pod <pod-name> -n file-simulator`
2. Check init container logs: `kubectl logs <pod-name> -c sync-windows-data -n file-simulator`
3. Check main container logs: `kubectl logs <pod-name> -c nfs-server -n file-simulator`
4. Verify Windows directories exist: `ls C:\simulator-data\`
  </action>
  <verify>
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o wide
# All 7 pods should be Running with 1/1 READY

kubectl get svc -n file-simulator -l simulator.protocol=nfs
# All 7 services should exist with NodePorts
```
  </verify>
  <done>All 7 NAS pods (nas-input-1/2/3, nas-backup, nas-output-1/2/3) running with 1/1 READY status</done>
</task>

<task type="auto">
  <name>Task 2: Validate Storage Isolation and Subdirectory Mounts (EXP-01)</name>
  <files>None (validation only)</files>
  <action>
Verify that each NAS server sees only its own files and that subdirectories are correctly mounted.

**Create unique test files on Windows (one per NAS):**
```powershell
$nasServers = @("nas-input-1", "nas-input-2", "nas-input-3", "nas-backup", "nas-output-1", "nas-output-2", "nas-output-3")
foreach ($nas in $nasServers) {
    Set-Content -Path "C:\simulator-data\$nas\isolation-test-$nas.txt" -Value "This file should ONLY be visible in $nas"
}
```

**Create subdirectory structure for EXP-01 validation:**
```powershell
# Create subdirectories in nas-input-1 to test EXP-01
New-Item -ItemType Directory -Force -Path "C:\simulator-data\nas-input-1\sub-1"
New-Item -ItemType Directory -Force -Path "C:\simulator-data\nas-input-1\sub-1\nested"
Set-Content -Path "C:\simulator-data\nas-input-1\sub-1\subdir-test.txt" -Value "File in subdirectory sub-1"
Set-Content -Path "C:\simulator-data\nas-input-1\sub-1\nested\deep-test.txt" -Value "File in nested subdirectory"
```

**Restart all NAS pods to trigger init container sync:**
```bash
kubectl delete pods -n file-simulator -l simulator.protocol=nfs
kubectl wait --for=condition=Ready pod -l simulator.protocol=nfs -n file-simulator --timeout=180s
```

**Verify each pod sees only its own test file:**
```bash
# For each NAS server, count txt files in /data
# Each should see exactly 2 files: README.txt + isolation-test-{name}.txt

for nas in nas-input-1 nas-input-2 nas-input-3 nas-backup nas-output-1 nas-output-2 nas-output-3; do
  echo "=== $nas ==="
  POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=$nas -o jsonpath='{.items[0].metadata.name}')
  kubectl exec -n file-simulator $POD -- ls -la /data
  echo "File count: $(kubectl exec -n file-simulator $POD -- sh -c 'ls /data/*.txt 2>/dev/null | wc -l')"
  echo ""
done
```

**Verify subdirectory mount (EXP-01):**
```bash
# nas-input-1 should see subdirectory structure
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')

# Check subdirectory exists
kubectl exec -n file-simulator $POD -- ls -la /data/sub-1/
# Should show: subdir-test.txt, nested/

# Check nested subdirectory
kubectl exec -n file-simulator $POD -- ls -la /data/sub-1/nested/
# Should show: deep-test.txt

# Read file from subdirectory
kubectl exec -n file-simulator $POD -- cat /data/sub-1/subdir-test.txt
# Should output: "File in subdirectory sub-1"

# Read file from nested subdirectory
kubectl exec -n file-simulator $POD -- cat /data/sub-1/nested/deep-test.txt
# Should output: "File in nested subdirectory"
```

**Isolation test (critical):**
- nas-input-1 should NOT see isolation-test-nas-input-2.txt
- Each pod should see exactly 2 .txt files in root (README + its own isolation test)
- nas-input-1 should see sub-1/ directory with nested content

**If isolation fails:**
- Check hostPath in deployment: `kubectl get deployment -n file-simulator -o yaml | grep "path:"`
- Verify each deployment has unique path
  </action>
  <verify>
Each NAS pod shows exactly 2 .txt files in root (README.txt and its own isolation-test file).
No pod shows files from other NAS servers.
nas-input-1 shows sub-1/subdir-test.txt and sub-1/nested/deep-test.txt (EXP-01).
  </verify>
  <done>Storage isolation verified - each NAS sees only its own directory contents; subdirectory mounts work (EXP-01)</done>
</task>

<task type="auto">
  <name>Task 3: Validate Runtime Subdirectory Creation (EXP-02)</name>
  <files>None (validation only)</files>
  <action>
Verify that subdirectories created at runtime via kubectl exec persist across pod restarts.

**Create runtime subdirectory inside running pod:**
```bash
# Get nas-input-1 pod name
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')

# Create new subdirectory at runtime (this goes to emptyDir, NOT Windows)
kubectl exec -n file-simulator $POD -- mkdir -p /data/runtime-created
kubectl exec -n file-simulator $POD -- sh -c 'echo "Created at runtime" > /data/runtime-created/runtime-file.txt'

# Verify it exists
kubectl exec -n file-simulator $POD -- cat /data/runtime-created/runtime-file.txt
# Should output: "Created at runtime"
```

**Test persistence across pod restart:**
```bash
# Note: With current init-container sync pattern, runtime changes are LOST on restart
# because init container re-syncs from Windows, overwriting emptyDir

# Delete just nas-input-1 pod to trigger restart
kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=nas-input-1 -n file-simulator --timeout=60s

# Get new pod name
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')

# Check if runtime-created directory persists (expected: NO with current pattern)
kubectl exec -n file-simulator $POD -- ls /data/
# runtime-created should NOT exist (expected behavior for input NAS)
```

**Alternative: Create directory on Windows side for persistence:**
```powershell
# This is the correct way to persist subdirectories
New-Item -ItemType Directory -Force -Path "C:\simulator-data\nas-input-1\persistent-subdir"
Set-Content -Path "C:\simulator-data\nas-input-1\persistent-subdir\persisted.txt" -Value "This will persist"
```

```bash
# Restart pod to sync
kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=nas-input-1 -n file-simulator --timeout=60s

# Verify Windows-created subdirectory persists
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n file-simulator $POD -- cat /data/persistent-subdir/persisted.txt
# Should output: "This will persist"
```

**Document EXP-02 behavior:**
- Runtime-created directories in pod: Lost on restart (expected for input NAS - init container overwrites)
- Windows-created directories: Persist across restarts (synced by init container)
- For output NAS (Phase 3): Bidirectional sync will preserve runtime-created directories
  </action>
  <verify>
Runtime subdirectory creation via kubectl exec works.
Runtime directories are lost on pod restart (expected for input NAS with current pattern).
Windows-created subdirectories persist across pod restarts.
EXP-02 behavior documented for current architecture.
  </verify>
  <done>Runtime subdirectory creation tested (EXP-02); behavior documented - Windows-side directories persist, pod-side directories reset on restart (expected for input NAS)</done>
</task>

<task type="auto">
  <name>Task 4: Validate Multi-NAS Mount Capability (INT-03)</name>
  <files>None (validation only)</files>
  <action>
Verify that a single test pod can mount multiple NAS servers simultaneously, validating the production use case.

**Create a test pod that mounts 3 NAS servers:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: multi-nas-client
  namespace: file-simulator
spec:
  containers:
  - name: client
    image: alpine:latest
    command: ["sleep", "3600"]
    volumeMounts:
    - name: nas-input-1
      mountPath: /mnt/input-1
    - name: nas-input-2
      mountPath: /mnt/input-2
    - name: nas-output-1
      mountPath: /mnt/output-1
  volumes:
  - name: nas-input-1
    nfs:
      server: file-sim-file-simulator-nas-input-1.file-simulator.svc.cluster.local
      path: /data
  - name: nas-input-2
    nfs:
      server: file-sim-file-simulator-nas-input-2.file-simulator.svc.cluster.local
      path: /data
  - name: nas-output-1
    nfs:
      server: file-sim-file-simulator-nas-output-1.file-simulator.svc.cluster.local
      path: /data
EOF
```

**Wait for test pod to be ready:**
```bash
kubectl wait --for=condition=Ready pod/multi-nas-client -n file-simulator --timeout=120s
```

**Verify access to all 3 NAS servers from single pod:**
```bash
# Check nas-input-1 mount
kubectl exec -n file-simulator multi-nas-client -- ls /mnt/input-1/
# Should show: README.txt, isolation-test-nas-input-1.txt, sub-1/, persistent-subdir/

# Check nas-input-2 mount
kubectl exec -n file-simulator multi-nas-client -- ls /mnt/input-2/
# Should show: README.txt, isolation-test-nas-input-2.txt

# Check nas-output-1 mount
kubectl exec -n file-simulator multi-nas-client -- ls /mnt/output-1/
# Should show: README.txt, isolation-test-nas-output-1.txt

# Verify isolation: files from input-1 NOT in input-2
kubectl exec -n file-simulator multi-nas-client -- sh -c 'cat /mnt/input-1/isolation-test-nas-input-1.txt'
# Should output: "This file should ONLY be visible in nas-input-1"

kubectl exec -n file-simulator multi-nas-client -- sh -c 'ls /mnt/input-2/ | grep nas-input-1'
# Should return empty (no input-1 files in input-2 mount)
```

**Cross-NAS file operation test:**
```bash
# Write to output-1 (simulate microservice output)
kubectl exec -n file-simulator multi-nas-client -- sh -c 'echo "Processed data" > /mnt/output-1/processed-result.txt'

# Verify write succeeded
kubectl exec -n file-simulator multi-nas-client -- cat /mnt/output-1/processed-result.txt
# Should output: "Processed data"

# Read from input-1, process, write to output-1 (simulate real workflow)
kubectl exec -n file-simulator multi-nas-client -- sh -c 'cat /mnt/input-1/README.txt | tr "a-z" "A-Z" > /mnt/output-1/transformed.txt'
kubectl exec -n file-simulator multi-nas-client -- cat /mnt/output-1/transformed.txt
# Should output: uppercase version of input-1 README
```

**Cleanup test pod:**
```bash
kubectl delete pod/multi-nas-client -n file-simulator
```
  </action>
  <verify>
Test pod successfully mounts 3 NAS servers simultaneously.
Each mount shows correct isolated content.
Read/write operations work across different NAS mounts.
No cross-contamination between NAS servers when accessed from same pod.
  </verify>
  <done>Multi-NAS mount capability validated (INT-03); single pod can mount and access multiple NAS servers with proper isolation</done>
</task>

<task type="auto">
  <name>Task 5: Create Multi-NAS Validation Script</name>
  <files>scripts/test-multi-nas.ps1</files>
  <action>
Create comprehensive PowerShell test script for 7-server topology validation.

**Script structure (follow test-nas-pattern.ps1 pattern):**

```powershell
# test-multi-nas.ps1
# Phase 2: 7-Server NAS Topology Validation Script

param(
    [switch]$CreateTestFiles,
    [switch]$SkipDeployment
)

$ErrorActionPreference = "Stop"

$nasServers = @(
    @{name="nas-input-1"; nodePort=32150},
    @{name="nas-input-2"; nodePort=32151},
    @{name="nas-input-3"; nodePort=32152},
    @{name="nas-backup"; nodePort=32153},
    @{name="nas-output-1"; nodePort=32154},
    @{name="nas-output-2"; nodePort=32155},
    @{name="nas-output-3"; nodePort=32156}
)

function Write-Step { param($step, $description) Write-Host "`n=== Step $step`: $description ===" -ForegroundColor Cyan }
function Write-Pass { param($message) Write-Host "[PASS] $message" -ForegroundColor Green }
function Write-Fail { param($message) Write-Host "[FAIL] $message" -ForegroundColor Red }
function Write-Info { param($message) Write-Host "[INFO] $message" -ForegroundColor Yellow }
```

**Test steps to include:**
1. Minikube status check
2. Verify Windows directories exist (create if -CreateTestFiles)
3. Helm deployment status
4. All 7 pods running check
5. Init container success check (per pod)
6. Storage isolation test (critical)
7. **Subdirectory mount test (EXP-01)** - verify sub-1/nested structure visible
8. **Runtime subdirectory test (EXP-02)** - create, verify, document persistence behavior
9. DNS resolution test (from within cluster)
10. NodePort accessibility test
11. **Multi-NAS mount test (INT-03)** - deploy test pod, verify 3 simultaneous mounts
12. Resource usage check (kubectl top pods)
13. Summary with pass/fail count

**Critical tests:**
- Storage isolation (each pod sees only its directory)
- Subdirectory mounts (EXP-01)
- Multi-NAS mount capability (INT-03)
- All 7 pods Running
- No privileged security context

**Output format:**
```
=== Step 1: Minikube Status ===
[PASS] Minikube is running
[PASS] Mount active: /mnt/simulator-data

=== Step 6: Storage Isolation ===
[PASS] nas-input-1: 2 files (isolated)
[PASS] nas-input-2: 2 files (isolated)
...

=== Step 7: Subdirectory Mounts (EXP-01) ===
[PASS] nas-input-1: sub-1/subdir-test.txt accessible
[PASS] nas-input-1: sub-1/nested/deep-test.txt accessible

=== Step 11: Multi-NAS Mount (INT-03) ===
[PASS] Test pod deployed with 3 NAS mounts
[PASS] /mnt/input-1 accessible
[PASS] /mnt/input-2 accessible
[PASS] /mnt/output-1 accessible
[PASS] Cross-NAS isolation verified

=== Summary ===
Passed: 35/38 tests
Failed: 3 tests
- DNS resolution requires rpcbind (deferred to Phase 3)
- External NFS mount not tested (Phase 2 limitation)
- Runtime subdirs lost on restart (expected for input NAS)
```
  </action>
  <verify>
```powershell
# Run the script
.\scripts\test-multi-nas.ps1

# Script should complete without errors
# All critical tests should pass
```
  </verify>
  <done>test-multi-nas.ps1 script created and validates 7-server topology including EXP-01, EXP-02, INT-03</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
7-server NAS topology deployed to Minikube with:
- 7 independent NAS pods (nas-input-1/2/3, nas-backup, nas-output-1/2/3)
- Unique NodePorts (32150-32156) for each server
- Storage isolation (each NAS sees only its Windows directory)
- Subdirectory mount support (EXP-01)
- Runtime subdirectory creation tested (EXP-02)
- Multi-NAS mount capability validated (INT-03)
- Non-privileged security context
- test-multi-nas.ps1 validation script
  </what-built>
  <how-to-verify>
1. Run the validation script:
   ```powershell
   .\scripts\test-multi-nas.ps1
   ```

2. Verify all 7 pods are running:
   ```bash
   kubectl get pods -n file-simulator -l simulator.protocol=nfs
   ```

3. Test storage isolation manually:
   - Create a file in C:\simulator-data\nas-input-1\test.txt
   - Restart nas-input-1 pod: `kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1`
   - Verify file appears: `kubectl exec -n file-simulator <pod> -- cat /data/test.txt`
   - Verify file NOT in nas-input-2: `kubectl exec -n file-simulator <nas-input-2-pod> -- ls /data/`

4. Test subdirectory mounts (EXP-01):
   ```bash
   POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')
   kubectl exec -n file-simulator $POD -- cat /data/sub-1/nested/deep-test.txt
   ```

5. Test multi-NAS mount (INT-03):
   - Verify multi-nas-client test pod can access all 3 mounts
   - Or re-run the multi-NAS test from Task 4

6. Check resource usage:
   ```bash
   kubectl top pods -n file-simulator -l simulator.protocol=nfs
   ```
   Total should be under 4GB RAM / 2 CPU (50% of Minikube capacity)

7. Verify NodePort accessibility (from Windows):
   ```powershell
   $minikubeIP = minikube ip
   # Each port should respond (may timeout without rpcbind, but port should be listening)
   Test-NetConnection -ComputerName $minikubeIP -Port 32150
   ```
  </how-to-verify>
  <resume-signal>Type "approved" if all 7 NAS servers are running with isolated storage, subdirectory support (EXP-01), and multi-mount capability (INT-03), or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
**Deployment Status:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o wide
kubectl get svc -n file-simulator -l simulator.protocol=nfs
```

**Storage Isolation (must pass):**
```bash
# Each pod should see exactly its own files
for nas in nas-input-1 nas-input-2 nas-input-3 nas-backup nas-output-1 nas-output-2 nas-output-3; do
  POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=$nas -o jsonpath='{.items[0].metadata.name}')
  echo "$nas: $(kubectl exec -n file-simulator $POD -- sh -c 'ls /data/ | wc -l') files"
done
```

**Subdirectory Mounts (EXP-01):**
```bash
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n file-simulator $POD -- ls -laR /data/sub-1/
```

**Multi-NAS Mount (INT-03):**
```bash
# Deploy test pod and verify 3 simultaneous mounts work
kubectl get pod/multi-nas-client -n file-simulator
```

**Resource Usage:**
```bash
kubectl top pods -n file-simulator -l simulator.protocol=nfs
# Total should be under 500Mi RAM (well within 8GB Minikube)
```

**Security Context:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o jsonpath='{range .items[*]}{.metadata.name}: privileged={.spec.containers[0].securityContext.privileged}{"\n"}{end}'
# Should show no "privileged=true"
```
</verification>

<success_criteria>
1. All 7 NAS pods deployed and Running (1/1 READY)
2. Each NAS server has unique NodePort (32150-32156) accessible
3. Storage isolation verified (nas-input-1 files NOT visible in nas-input-2)
4. Each NAS has predictable DNS name within cluster
5. Total resource usage under 50% of Minikube capacity (4GB/2CPU)
6. No privileged security context on any NAS pod
7. Subdirectory mounts work (EXP-01) - Windows subdirs visible via NFS
8. Runtime subdirectory creation tested (EXP-02) - behavior documented
9. Multi-NAS mount capability validated (INT-03) - single pod mounts 3+ NAS servers
10. test-multi-nas.ps1 script validates full topology
11. Human verification confirms system works as expected
</success_criteria>

<output>
After completion, create `.planning/phases/02-7-server-topology/02-02-SUMMARY.md`
</output>
