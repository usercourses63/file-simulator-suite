---
phase: 02-7-server-topology
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - scripts/test-multi-nas.ps1
autonomous: false

must_haves:
  truths:
    - "Runtime-created subdirectories persist across pod restarts (EXP-02)"
    - "Single pod can mount multiple NAS servers simultaneously (INT-03)"
    - "All 7 servers fit within Minikube 8GB/4CPU resource constraints"
    - "Each NAS has unique DNS name resolvable within cluster"
  artifacts:
    - path: "scripts/test-multi-nas.ps1"
      provides: "Automated validation script for 7-server topology"
      min_lines: 100
      contains: "nas-input-1"
  key_links:
    - from: "multi-nas-client pod"
      to: "3 NAS services"
      via: "NFS volume mounts"
      pattern: "nfs:.*server:.*svc.cluster.local"
---

<objective>
Validate advanced NAS topology features: runtime subdirectory persistence (EXP-02), multi-NAS mount capability (INT-03), and create automated validation script.

Purpose: Complete Phase 2 validation with production-readiness tests and automation for future regression testing.

Output: Validated EXP-02/INT-03 behavior + test-multi-nas.ps1 automation script.
</objective>

<execution_context>
@C:\Users\UserC\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\UserC\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-7-server-topology/02-RESEARCH.md
@.planning/phases/02-7-server-topology/02-01-SUMMARY.md
@.planning/phases/02-7-server-topology/02-02-SUMMARY.md

# Phase 1 test script pattern
@scripts/test-nas-pattern.ps1

# Values file for deployed topology
@helm-chart/file-simulator/values-multi-nas.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Validate Runtime Subdirectory Creation (EXP-02)</name>
  <files>None (validation only)</files>
  <action>
Verify that subdirectories created at runtime via kubectl exec persist across pod restarts.

**Create runtime subdirectory inside running pod:**
```bash
# Get nas-input-1 pod name
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')

# Create new subdirectory at runtime (this goes to emptyDir, NOT Windows)
kubectl exec -n file-simulator $POD -- mkdir -p /data/runtime-created
kubectl exec -n file-simulator $POD -- sh -c 'echo "Created at runtime" > /data/runtime-created/runtime-file.txt'

# Verify it exists
kubectl exec -n file-simulator $POD -- cat /data/runtime-created/runtime-file.txt
# Should output: "Created at runtime"
```

**Test persistence across pod restart:**
```bash
# Note: With current init-container sync pattern, runtime changes are LOST on restart
# because init container re-syncs from Windows, overwriting emptyDir

# Delete just nas-input-1 pod to trigger restart
kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=nas-input-1 -n file-simulator --timeout=60s

# Get new pod name
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')

# Check if runtime-created directory persists (expected: NO with current pattern)
kubectl exec -n file-simulator $POD -- ls /data/
# runtime-created should NOT exist (expected behavior for input NAS)
```

**Alternative: Create directory on Windows side for persistence:**
```powershell
# This is the correct way to persist subdirectories
New-Item -ItemType Directory -Force -Path "C:\simulator-data\nas-input-1\persistent-subdir"
Set-Content -Path "C:\simulator-data\nas-input-1\persistent-subdir\persisted.txt" -Value "This will persist"
```

```bash
# Restart pod to sync
kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/component=nas-input-1 -n file-simulator --timeout=60s

# Verify Windows-created subdirectory persists
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n file-simulator $POD -- cat /data/persistent-subdir/persisted.txt
# Should output: "This will persist"
```

**Document EXP-02 behavior:**
- Runtime-created directories in pod: Lost on restart (expected for input NAS - init container overwrites)
- Windows-created directories: Persist across restarts (synced by init container)
- For output NAS (Phase 3): Bidirectional sync will preserve runtime-created directories
  </action>
  <verify>
Runtime subdirectory creation via kubectl exec works.
Runtime directories are lost on pod restart (expected for input NAS with current pattern).
Windows-created subdirectories persist across pod restarts.
EXP-02 behavior documented for current architecture.
  </verify>
  <done>Runtime subdirectory creation tested (EXP-02); behavior documented - Windows-side directories persist, pod-side directories reset on restart (expected for input NAS)</done>
</task>

<task type="auto">
  <name>Task 2: Validate Multi-NAS Mount Capability (INT-03)</name>
  <files>None (validation only)</files>
  <action>
Verify that a single test pod can mount multiple NAS servers simultaneously, validating the production use case.

**Create a test pod that mounts 3 NAS servers:**
```bash
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: multi-nas-client
  namespace: file-simulator
spec:
  containers:
  - name: client
    image: alpine:latest
    command: ["sleep", "3600"]
    volumeMounts:
    - name: nas-input-1
      mountPath: /mnt/input-1
    - name: nas-input-2
      mountPath: /mnt/input-2
    - name: nas-output-1
      mountPath: /mnt/output-1
  volumes:
  - name: nas-input-1
    nfs:
      server: file-sim-file-simulator-nas-input-1.file-simulator.svc.cluster.local
      path: /data
  - name: nas-input-2
    nfs:
      server: file-sim-file-simulator-nas-input-2.file-simulator.svc.cluster.local
      path: /data
  - name: nas-output-1
    nfs:
      server: file-sim-file-simulator-nas-output-1.file-simulator.svc.cluster.local
      path: /data
EOF
```

**Wait for test pod to be ready:**
```bash
kubectl wait --for=condition=Ready pod/multi-nas-client -n file-simulator --timeout=120s
```

**Verify access to all 3 NAS servers from single pod:**
```bash
# Check nas-input-1 mount
kubectl exec -n file-simulator multi-nas-client -- ls /mnt/input-1/
# Should show: README.txt, isolation-test-nas-input-1.txt, sub-1/, persistent-subdir/

# Check nas-input-2 mount
kubectl exec -n file-simulator multi-nas-client -- ls /mnt/input-2/
# Should show: README.txt, isolation-test-nas-input-2.txt

# Check nas-output-1 mount
kubectl exec -n file-simulator multi-nas-client -- ls /mnt/output-1/
# Should show: README.txt, isolation-test-nas-output-1.txt

# Verify isolation: files from input-1 NOT in input-2
kubectl exec -n file-simulator multi-nas-client -- sh -c 'cat /mnt/input-1/isolation-test-nas-input-1.txt'
# Should output: "This file should ONLY be visible in nas-input-1"

kubectl exec -n file-simulator multi-nas-client -- sh -c 'ls /mnt/input-2/ | grep nas-input-1'
# Should return empty (no input-1 files in input-2 mount)
```

**Cross-NAS file operation test:**
```bash
# Write to output-1 (simulate microservice output)
kubectl exec -n file-simulator multi-nas-client -- sh -c 'echo "Processed data" > /mnt/output-1/processed-result.txt'

# Verify write succeeded
kubectl exec -n file-simulator multi-nas-client -- cat /mnt/output-1/processed-result.txt
# Should output: "Processed data"

# Read from input-1, process, write to output-1 (simulate real workflow)
kubectl exec -n file-simulator multi-nas-client -- sh -c 'cat /mnt/input-1/README.txt | tr "a-z" "A-Z" > /mnt/output-1/transformed.txt'
kubectl exec -n file-simulator multi-nas-client -- cat /mnt/output-1/transformed.txt
# Should output: uppercase version of input-1 README
```

**Cleanup test pod:**
```bash
kubectl delete pod/multi-nas-client -n file-simulator
```
  </action>
  <verify>
Test pod successfully mounts 3 NAS servers simultaneously.
Each mount shows correct isolated content.
Read/write operations work across different NAS mounts.
No cross-contamination between NAS servers when accessed from same pod.
  </verify>
  <done>Multi-NAS mount capability validated (INT-03); single pod can mount and access multiple NAS servers with proper isolation</done>
</task>

<task type="auto">
  <name>Task 3: Create Multi-NAS Validation Script</name>
  <files>scripts/test-multi-nas.ps1</files>
  <action>
Create comprehensive PowerShell test script for 7-server topology validation.

**Script structure (follow test-nas-pattern.ps1 pattern):**

```powershell
# test-multi-nas.ps1
# Phase 2: 7-Server NAS Topology Validation Script

param(
    [switch]$CreateTestFiles,
    [switch]$SkipDeployment
)

$ErrorActionPreference = "Stop"

$nasServers = @(
    @{name="nas-input-1"; nodePort=32150},
    @{name="nas-input-2"; nodePort=32151},
    @{name="nas-input-3"; nodePort=32152},
    @{name="nas-backup"; nodePort=32153},
    @{name="nas-output-1"; nodePort=32154},
    @{name="nas-output-2"; nodePort=32155},
    @{name="nas-output-3"; nodePort=32156}
)

function Write-Step { param($step, $description) Write-Host "`n=== Step $step`: $description ===" -ForegroundColor Cyan }
function Write-Pass { param($message) Write-Host "[PASS] $message" -ForegroundColor Green }
function Write-Fail { param($message) Write-Host "[FAIL] $message" -ForegroundColor Red }
function Write-Info { param($message) Write-Host "[INFO] $message" -ForegroundColor Yellow }
```

**Test steps to include:**
1. Minikube status check
2. Verify Windows directories exist (create if -CreateTestFiles)
3. Helm deployment status
4. All 7 pods running check
5. Init container success check (per pod)
6. Storage isolation test (critical)
7. **Subdirectory mount test (EXP-01)** - verify sub-1/nested structure visible
8. **Runtime subdirectory test (EXP-02)** - create, verify, document persistence behavior
9. DNS resolution test (from within cluster)
10. NodePort accessibility test
11. **Multi-NAS mount test (INT-03)** - deploy test pod, verify 3 simultaneous mounts
12. Resource usage check (kubectl top pods)
13. Summary with pass/fail count

**Critical tests:**
- Storage isolation (each pod sees only its directory)
- Subdirectory mounts (EXP-01)
- Multi-NAS mount capability (INT-03)
- All 7 pods Running
- No privileged security context

**Output format:**
```
=== Step 1: Minikube Status ===
[PASS] Minikube is running
[PASS] Mount active: /mnt/simulator-data

=== Step 6: Storage Isolation ===
[PASS] nas-input-1: 2 files (isolated)
[PASS] nas-input-2: 2 files (isolated)
...

=== Step 7: Subdirectory Mounts (EXP-01) ===
[PASS] nas-input-1: sub-1/subdir-test.txt accessible
[PASS] nas-input-1: sub-1/nested/deep-test.txt accessible

=== Step 8: Runtime Subdirectory (EXP-02) ===
[PASS] Runtime directory created in pod
[INFO] Runtime directories reset on pod restart (expected for input NAS)
[PASS] Windows-created directories persist across restarts

=== Step 11: Multi-NAS Mount (INT-03) ===
[PASS] Test pod deployed with 3 NAS mounts
[PASS] /mnt/input-1 accessible
[PASS] /mnt/input-2 accessible
[PASS] /mnt/output-1 accessible
[PASS] Cross-NAS isolation verified

=== Summary ===
Passed: 32/32 tests
Failed: 0 tests
```
  </action>
  <verify>
```powershell
# Run the script
.\scripts\test-multi-nas.ps1

# Script should complete without errors
# All critical tests should pass
```
  </verify>
  <done>test-multi-nas.ps1 script created and validates 7-server topology including EXP-01, EXP-02, INT-03</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
7-server NAS topology deployed to Minikube with:
- 7 independent NAS pods (nas-input-1/2/3, nas-backup, nas-output-1/2/3)
- Unique NodePorts (32150-32156) for each server
- Storage isolation (each NAS sees only its Windows directory)
- Subdirectory mount support (EXP-01)
- Runtime subdirectory creation tested (EXP-02)
- Multi-NAS mount capability validated (INT-03)
- Non-privileged security context
- test-multi-nas.ps1 validation script
  </what-built>
  <how-to-verify>
1. Run the validation script:
   ```powershell
   .\scripts\test-multi-nas.ps1
   ```

2. Verify all 7 pods are running:
   ```bash
   kubectl get pods -n file-simulator -l simulator.protocol=nfs
   ```

3. Test storage isolation manually:
   - Create a file in C:\simulator-data\nas-input-1\test.txt
   - Restart nas-input-1 pod: `kubectl delete pod -n file-simulator -l app.kubernetes.io/component=nas-input-1`
   - Verify file appears: `kubectl exec -n file-simulator <pod> -- cat /data/test.txt`
   - Verify file NOT in nas-input-2: `kubectl exec -n file-simulator <nas-input-2-pod> -- ls /data/`

4. Test subdirectory mounts (EXP-01):
   ```bash
   POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')
   kubectl exec -n file-simulator $POD -- cat /data/sub-1/nested/deep-test.txt
   ```

5. Test multi-NAS mount (INT-03):
   - Verify multi-nas-client test pod can access all 3 mounts
   - Or re-run the multi-NAS test from Task 2

6. Check resource usage:
   ```bash
   kubectl top pods -n file-simulator -l simulator.protocol=nfs
   ```
   Total should be under 4GB RAM / 2 CPU (50% of Minikube capacity)

7. Verify NodePort accessibility (from Windows):
   ```powershell
   $minikubeIP = minikube ip
   # Each port should respond (may timeout without rpcbind, but port should be listening)
   Test-NetConnection -ComputerName $minikubeIP -Port 32150
   ```
  </how-to-verify>
  <resume-signal>Type "approved" if all 7 NAS servers are running with isolated storage, subdirectory support (EXP-01), and multi-mount capability (INT-03), or describe any issues found</resume-signal>
</task>

</tasks>

<verification>
**Runtime Subdirectory (EXP-02):**
```bash
# Verify behavior matches documentation
POD=$(kubectl get pod -n file-simulator -l app.kubernetes.io/component=nas-input-1 -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n file-simulator $POD -- ls /data/persistent-subdir/
# Should show persisted.txt (Windows-created persists)
```

**Multi-NAS Mount (INT-03):**
```bash
# Deploy test pod and verify 3 simultaneous mounts work
kubectl get pod/multi-nas-client -n file-simulator
```

**Resource Usage:**
```bash
kubectl top pods -n file-simulator -l simulator.protocol=nfs
# Total should be under 500Mi RAM (well within 8GB Minikube)
```

**Security Context:**
```bash
kubectl get pods -n file-simulator -l simulator.protocol=nfs -o jsonpath='{range .items[*]}{.metadata.name}: privileged={.spec.containers[0].securityContext.privileged}{"\n"}{end}'
# Should show no "privileged=true"
```
</verification>

<success_criteria>
1. Runtime subdirectory creation tested (EXP-02) - behavior documented
2. Multi-NAS mount capability validated (INT-03) - single pod mounts 3+ NAS servers
3. Total resource usage under 50% of Minikube capacity (4GB/2CPU)
4. Each NAS has predictable DNS name within cluster
5. test-multi-nas.ps1 script validates full topology
6. Human verification confirms system works as expected
</success_criteria>

<output>
After completion, create `.planning/phases/02-7-server-topology/02-03-SUMMARY.md`
</output>
